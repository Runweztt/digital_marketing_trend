{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10080b-7bef-4b43-804f-c671f45a0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.offline as pyo\n",
    "\n",
    "pyo.init_notebook_mode(connected=True)  # Initialize Plotly in Jupyter Notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for clean output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4472727-cefa-43bf-9f84-c2b732895fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/5: ['digital marketing', 'social media', 'content marketing', 'SEO', 'PPC']\n",
      "✓ Successfully retrieved data for: ['digital marketing', 'social media', 'content marketing', 'SEO', 'PPC']\n",
      "  Waiting 62.0 seconds before next batch...\n",
      "Processing batch 2/5: ['email marketing', 'influencer marketing', 'ecommerce', 'video marketing', 'affiliate marketing']\n",
      "✓ Successfully retrieved data for: ['email marketing', 'influencer marketing', 'ecommerce', 'video marketing', 'affiliate marketing']\n",
      "  Waiting 69.8 seconds before next batch...\n",
      "Processing batch 3/5: ['online advertising', 'SEM', 'Google Ads', 'Facebook Ads', 'content creation']\n",
      "✗ Error on attempt 1/3 for ['online advertising', 'SEM', 'Google Ads', 'Facebook Ads', 'content creation']: The request failed: Google returned a response with code 429\n",
      "  Waiting 125.4 seconds before retry...\n",
      "✓ Successfully retrieved data for: ['online advertising', 'SEM', 'Google Ads', 'Facebook Ads', 'content creation']\n",
      "  Waiting 66.1 seconds before next batch...\n",
      "Processing batch 4/5: ['brand strategy', 'customer engagement', 'market research', 'analytics', 'data-driven marketing']\n",
      "✓ Successfully retrieved data for: ['brand strategy', 'customer engagement', 'market research', 'analytics', 'data-driven marketing']\n",
      "  Waiting 65.3 seconds before next batch...\n",
      "Processing batch 5/5: ['user experience']\n",
      "✓ Successfully retrieved data for: ['user experience']\n",
      "  Waiting 66.8 seconds before next batch...\n",
      "Data collection complete! DataFrame shape: (90, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "import random\n",
    "\n",
    "# Set pandas option to avoid warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def get_trends_data(keywords, timeframe='now 7-d', geo='', retries=3, backoff_factor=1.5):\n",
    "    \"\"\"\n",
    "    Fetch Google Trends data for given keywords and timeframe with error handling and backoff.\n",
    "    \n",
    "    Parameters:\n",
    "    keywords (list): A list of keywords to query.\n",
    "    timeframe (str): The time range for the trends data.\n",
    "    geo (str): Geographic location (default is global).\n",
    "    retries (int): Number of retries if a request fails.\n",
    "    backoff_factor (float): Factor to increase wait time between retries.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Combined trends data for all keywords.\n",
    "    \"\"\"\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    # Process keywords in batches of 5 (Google Trends limit)\n",
    "    for i in range(0, len(keywords), 5):\n",
    "        keyword_batch = keywords[i:i+5]\n",
    "        batch_success = False\n",
    "        attempt = 0\n",
    "        wait_time = 60  # Initial wait time in seconds\n",
    "        \n",
    "        print(f\"Processing batch {i//5 + 1}/{(len(keywords)-1)//5 + 1}: {keyword_batch}\")\n",
    "        \n",
    "        # Retry logic\n",
    "        while not batch_success and attempt < retries:\n",
    "            try:\n",
    "                pytrends.build_payload(keyword_batch, timeframe=timeframe, geo=geo)\n",
    "                data = pytrends.interest_over_time()\n",
    "                \n",
    "                if not data.empty:\n",
    "                    # Drop isPartial column if it exists\n",
    "                    if 'isPartial' in data.columns:\n",
    "                        data = data.drop('isPartial', axis=1)\n",
    "                    \n",
    "                    # If this is our first successful batch\n",
    "                    if all_data.empty:\n",
    "                        all_data = data\n",
    "                    else:\n",
    "                        # Merge with existing data on date index\n",
    "                        all_data = pd.merge(all_data, data, left_index=True, right_index=True, how='outer')\n",
    "                    \n",
    "                    batch_success = True\n",
    "                    print(f\"✓ Successfully retrieved data for: {keyword_batch}\")\n",
    "                else:\n",
    "                    print(f\"✗ No data returned for: {keyword_batch}\")\n",
    "                    attempt += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"✗ Error on attempt {attempt}/{retries} for {keyword_batch}: {str(e)}\")\n",
    "                \n",
    "            # If we need to retry, wait with exponential backoff plus some randomization\n",
    "            if not batch_success and attempt < retries:\n",
    "                wait_time = wait_time * backoff_factor * (1 + random.random() * 0.5)\n",
    "                print(f\"  Waiting {wait_time:.1f} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                # Always wait between successful batches to avoid rate limiting\n",
    "                jitter = random.random() * 10 + 60  # Between 60-70 seconds\n",
    "                print(f\"  Waiting {jitter:.1f} seconds before next batch...\")\n",
    "                time.sleep(jitter)\n",
    "    \n",
    "    # Fill NaN values with 0 (for keywords that weren't in all batches)\n",
    "    all_data = all_data.fillna(0)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\n",
    "        'digital marketing', 'social media', 'content marketing', 'SEO', 'PPC', \n",
    "        'email marketing', 'influencer marketing', 'ecommerce', 'video marketing', \n",
    "        'affiliate marketing', 'online advertising', 'SEM', 'Google Ads', 'Facebook Ads', \n",
    "        'content creation', 'brand strategy', 'customer engagement', 'market research', \n",
    "        'analytics', 'data-driven marketing', 'user experience'\n",
    "    ]\n",
    "    \n",
    "    # You can specify a geographical region if needed\n",
    "    timeframe = '2025-01-01 2025-03-31'\n",
    "    geo = ''  # Empty for worldwide, 'US' for United States, etc.\n",
    "    \n",
    "    trends_df = get_trends_data(keywords, timeframe, geo)\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    trends_df.to_csv('digital_marketing_trends_q1_2025.csv')\n",
    "    print(f\"Data collection complete! DataFrame shape: {trends_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbf5584-41f1-414c-b6d1-711faddebfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All visualizations saved to trends_visualizations/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def visualize_trends_data(trends_df, output_folder=\"trends_visualizations\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for Google Trends data.\n",
    "    \n",
    "    Parameters:\n",
    "    trends_df (pd.DataFrame): DataFrame containing Google Trends data\n",
    "    output_folder (str): Folder to save visualization images\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set the style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 8)})\n",
    "    \n",
    "    # 1. Line plot of all trends over time\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.title('Google Trends for Digital Marketing Keywords (Q1 2025)', fontsize=16)\n",
    "    \n",
    "    # Plot each trend line\n",
    "    for column in trends_df.columns:\n",
    "        plt.plot(trends_df.index, trends_df[column], linewidth=2, label=column)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Search Interest')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis to show dates nicely\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/all_trends_line_plot.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of correlation between trends\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    correlation = trends_df.corr()\n",
    "    mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "    \n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                square=True, linewidths=.5, annot=True, fmt=\".2f\", cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Correlation Between Digital Marketing Trends', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/correlation_heatmap.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Top 5 trending topics over time\n",
    "    top_5_keywords = trends_df.mean().nlargest(5).index\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for keyword in top_5_keywords:\n",
    "        plt.plot(trends_df.index, trends_df[keyword], linewidth=3, label=keyword)\n",
    "    \n",
    "    plt.title('Top 5 Digital Marketing Trends (Q1 2025)', fontsize=16)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Search Interest')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/top_5_trends.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Seasonal decomposition for top trend\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    \n",
    "    top_trend = trends_df.mean().idxmax()\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    try:\n",
    "        result = seasonal_decompose(trends_df[top_trend], model='additive', period=7)\n",
    "        result.plot()\n",
    "        plt.suptitle(f'Seasonal Decomposition of \"{top_trend}\" Trend', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.savefig(f\"{output_folder}/seasonal_decomposition.png\", dpi=300)\n",
    "        plt.close()\n",
    "    except:\n",
    "        print(\"Could not perform seasonal decomposition - may need more data points\")\n",
    "    \n",
    "    # 5. Bar chart of average interest over the period\n",
    "    avg_interest = trends_df.mean().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    bars = plt.bar(avg_interest.index, avg_interest.values, color=sns.color_palette(\"viridis\", len(avg_interest)))\n",
    "    \n",
    "    plt.title('Average Search Interest by Keyword (Q1 2025)', fontsize=16)\n",
    "    plt.xlabel('Keyword')\n",
    "    plt.ylabel('Average Interest')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f}', ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/average_interest_bar.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✓ All visualizations saved to {output_folder}/\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # If you've saved the data\n",
    "    try:\n",
    "        trends_df = pd.read_csv('digital_marketing_trends_q1_2025.csv', index_col=0, parse_dates=True)\n",
    "        visualize_trends_data(trends_df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run the data collection script first to generate the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba08c2ce-62fb-4ae1-9052-8348d4e94881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Insights report generated: digital_marketing_trends_insights.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_marketing_trends(trends_df):\n",
    "    \"\"\"\n",
    "    Perform simplified advanced analysis on Google Trends data for marketing insights.\n",
    "    Removed dependencies on sklearn for clustering and PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    trends_df (pd.DataFrame): DataFrame containing Google Trends data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing various analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    results['basic_stats'] = {\n",
    "        'highest_avg_interest': trends_df.mean().idxmax(),\n",
    "        'lowest_avg_interest': trends_df.mean().idxmin(),\n",
    "        'most_volatile': trends_df.std().idxmax(),\n",
    "        'most_stable': trends_df.std().idxmin(),\n",
    "    }\n",
    "    \n",
    "    # 2. Identify trending topics (keywords with positive slope)\n",
    "    # Calculate the slope of a linear fit for each keyword\n",
    "    slopes = {}\n",
    "    for column in trends_df.columns:\n",
    "        y = trends_df[column].values\n",
    "        x = np.arange(len(y))\n",
    "        slope, _ = np.polyfit(x, y, 1)\n",
    "        slopes[column] = slope\n",
    "    \n",
    "    trending_up = {k: v for k, v in slopes.items() if v > 0}\n",
    "    trending_down = {k: v for k, v in slopes.items() if v < 0}\n",
    "    \n",
    "    results['trend_direction'] = {\n",
    "        'trending_up': dict(sorted(trending_up.items(), key=lambda x: x[1], reverse=True)),\n",
    "        'trending_down': dict(sorted(trending_down.items(), key=lambda x: x[1]))\n",
    "    }\n",
    "    \n",
    "    # 3. Weekly patterns analysis\n",
    "    # Add day of week if the index is a DatetimeIndex\n",
    "    if isinstance(trends_df.index, pd.DatetimeIndex):\n",
    "        day_of_week = trends_df.index.dayofweek\n",
    "        weekly_patterns = {}\n",
    "        \n",
    "        for column in trends_df.columns:\n",
    "            # Create temporary dataframe with day of week\n",
    "            temp_df = pd.DataFrame({'value': trends_df[column], 'day_of_week': day_of_week})\n",
    "            day_avg = temp_df.groupby('day_of_week')['value'].mean()\n",
    "            peak_day = day_avg.idxmax()\n",
    "            day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            \n",
    "            # Calculate weekend vs weekday ratio\n",
    "            weekend_mask = day_of_week.isin([5, 6])  # 5, 6 are Saturday and Sunday\n",
    "            weekend_avg = trends_df[column][weekend_mask].mean()\n",
    "            weekday_avg = trends_df[column][~weekend_mask].mean()\n",
    "            weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0\n",
    "            \n",
    "            weekly_patterns[column] = {\n",
    "                'peak_day': day_names[peak_day],\n",
    "                'peak_value': day_avg.max(),\n",
    "                'low_day': day_names[day_avg.idxmin()],\n",
    "                'low_value': day_avg.min(),\n",
    "                'weekend_vs_weekday': weekend_ratio\n",
    "            }\n",
    "        \n",
    "        results['weekly_patterns'] = weekly_patterns\n",
    "    else:\n",
    "        results['weekly_patterns'] = {}\n",
    "        print(\"Warning: DataFrame index is not DatetimeIndex, skipping weekly patterns analysis\")\n",
    "    \n",
    "    # 4. Simple keyword grouping based on correlation (instead of K-means clustering)\n",
    "    corr_matrix = trends_df.corr()\n",
    "    \n",
    "    # Group keywords with similar correlation patterns\n",
    "    groups = {}\n",
    "    visited = set()\n",
    "    \n",
    "    for keyword in trends_df.columns:\n",
    "        if keyword in visited:\n",
    "            continue\n",
    "            \n",
    "        # Find highly correlated keywords\n",
    "        related = corr_matrix[keyword][corr_matrix[keyword] > 0.7].index.tolist()\n",
    "        \n",
    "        # Remove self-correlation\n",
    "        if keyword in related:\n",
    "            related.remove(keyword)\n",
    "            \n",
    "        # Add to group if there are related keywords\n",
    "        if related:\n",
    "            group_id = len(groups)\n",
    "            groups[group_id] = [keyword] + related\n",
    "            visited.add(keyword)\n",
    "            visited.update(related)\n",
    "        else:\n",
    "            # Create a group with just this keyword\n",
    "            group_id = len(groups)\n",
    "            groups[group_id] = [keyword]\n",
    "            visited.add(keyword)\n",
    "    \n",
    "    results['keyword_groups'] = groups\n",
    "    \n",
    "    # 5. Correlation-based recommendations\n",
    "    keyword_correlations = {}\n",
    "    for keyword in trends_df.columns:\n",
    "        correlations = corr_matrix[keyword].drop(keyword).nlargest(3)\n",
    "        keyword_correlations[keyword] = correlations.to_dict()\n",
    "    \n",
    "    results['related_keywords'] = keyword_correlations\n",
    "    \n",
    "    # 6. Create a heatmap visualization of keyword correlations\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                square=True, linewidths=.5, annot=False, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Correlation Between Digital Marketing Trends', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    import os\n",
    "    if not os.path.exists('trends_visualizations'):\n",
    "        os.makedirs('trends_visualizations')\n",
    "        \n",
    "    plt.savefig('trends_visualizations/keyword_correlation_analysis.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_insights_report(analysis_results, output_file=\"digital_marketing_trends_insights.md\"):\n",
    "    \"\"\"\n",
    "    Generate a markdown report from the analysis results.\n",
    "    \n",
    "    Parameters:\n",
    "    analysis_results (dict): Results from the analyze_marketing_trends function\n",
    "    output_file (str): Path to save the markdown report\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# Digital Marketing Trends Analysis (Q1 2025)\\n\\n\")\n",
    "        \n",
    "        # Basic stats section\n",
    "        f.write(\"## Key Findings\\n\\n\")\n",
    "        stats = analysis_results['basic_stats']\n",
    "        f.write(f\"- **Highest Average Interest:** {stats['highest_avg_interest']}\\n\")\n",
    "        f.write(f\"- **Lowest Average Interest:** {stats['lowest_avg_interest']}\\n\")\n",
    "        f.write(f\"- **Most Volatile Topic:** {stats['most_volatile']}\\n\")\n",
    "        f.write(f\"- **Most Stable Topic:** {stats['most_stable']}\\n\\n\")\n",
    "        \n",
    "        # Trend direction\n",
    "        f.write(\"## Trending Topics\\n\\n\")\n",
    "        f.write(\"### Topics Gaining Popularity\\n\\n\")\n",
    "        for topic, slope in list(analysis_results['trend_direction']['trending_up'].items())[:5]:\n",
    "            f.write(f\"- **{topic}** (slope: {slope:.4f})\\n\")\n",
    "        \n",
    "        f.write(\"\\n### Topics Declining in Interest\\n\\n\")\n",
    "        for topic, slope in list(analysis_results['trend_direction']['trending_down'].items())[:5]:\n",
    "            f.write(f\"- **{topic}** (slope: {slope:.4f})\\n\")\n",
    "        \n",
    "        # Weekly patterns\n",
    "        if analysis_results['weekly_patterns']:\n",
    "            f.write(\"\\n## Weekly Search Patterns\\n\\n\")\n",
    "            f.write(\"| Keyword | Peak Day | Weekend/Weekday Ratio |\\n\")\n",
    "            f.write(\"|---------|----------|----------------------|\\n\")\n",
    "            \n",
    "            for keyword, data in analysis_results['weekly_patterns'].items():\n",
    "                f.write(f\"| {keyword} | {data['peak_day']} | {data['weekend_vs_weekday']:.2f} |\\n\")\n",
    "        \n",
    "        # Keyword groups\n",
    "        f.write(\"\\n## Keyword Groups\\n\\n\")\n",
    "        f.write(\"These groups represent terms that show similar search patterns:\\n\\n\")\n",
    "        \n",
    "        for group_id, keywords in analysis_results['keyword_groups'].items():\n",
    "            f.write(f\"### Group {group_id+1}\\n\\n\")\n",
    "            for keyword in keywords:\n",
    "                f.write(f\"- {keyword}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Related keywords\n",
    "        f.write(\"## Related Keywords\\n\\n\")\n",
    "        f.write(\"For each keyword, here are the most closely related terms based on search patterns:\\n\\n\")\n",
    "        \n",
    "        for keyword, related in analysis_results['related_keywords'].items():\n",
    "            f.write(f\"### {keyword}\\n\\n\")\n",
    "            for related_keyword, correlation in related.items():\n",
    "                f.write(f\"- {related_keyword} (correlation: {correlation:.2f})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Marketing Recommendations\\n\\n\")\n",
    "        f.write(\"Based on this analysis, consider the following strategies:\\n\\n\")\n",
    "        \n",
    "        # Generate some basic recommendations\n",
    "        trending_topics = list(analysis_results['trend_direction']['trending_up'].keys())[:3]\n",
    "        f.write(f\"1. **Focus on rising trends**: Create content around {', '.join(trending_topics)}\\n\")\n",
    "        \n",
    "        # Find topics with strong weekend presence (if available)\n",
    "        if analysis_results['weekly_patterns']:\n",
    "            weekend_topics = sorted(\n",
    "                [(k, v['weekend_vs_weekday']) for k, v in analysis_results['weekly_patterns'].items()],\n",
    "                key=lambda x: x[1], reverse=True\n",
    "            )[:3]\n",
    "            \n",
    "            f.write(f\"2. **Weekend campaigns**: Target {', '.join([t[0] for t in weekend_topics])} on weekends\\n\")\n",
    "        \n",
    "        # Find complementary topics from groups\n",
    "        for group_id, keywords in analysis_results['keyword_groups'].items():\n",
    "            if len(keywords) >= 2:\n",
    "                f.write(f\"3. **Bundle related topics**: Create integrated campaigns that combine {' and '.join(keywords[:2])}\\n\")\n",
    "                break\n",
    "        \n",
    "        f.write(\"4. **Counter-cyclical opportunity**: Consider investing in declining topics with lower competition\\n\")\n",
    "    \n",
    "    print(f\"✓ Insights report generated: {output_file}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        trends_df = pd.read_csv('digital_marketing_trends_q1_2025.csv', index_col=0, parse_dates=True)\n",
    "        analysis_results = analyze_marketing_trends(trends_df)\n",
    "        generate_insights_report(analysis_results)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run the data collection script first to generate the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53132d8-5343-473a-ae22-b3df05fd4bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
